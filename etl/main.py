"""
Pipeline ETL principal pour le projet Electio-Analytics - H√©rault
Extraction, Transformation et Chargement des donn√©es √©lectorales et socio-√©conomiques
"""

import pandas as pd
from extract import create_spark_session, read_csv
from transform import (
    clean_column_names, 
    remove_duplicates, 
    handle_missing_values,
    normalize_commune_names,
    cast_columns_types,
    filter_department
)
from load import save_to_csv, save_to_parquet, create_summary_report
from pyspark.sql.types import IntegerType, DoubleType, StringType
import os

# Configuration
BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DATA_PATH = os.path.join(BASE_PATH, 'data', 'raw')
PROCESSED_DATA_PATH = r'C:\Users\theop\Desktop\data\clean'
FINAL_DATA_PATH = os.path.join(BASE_PATH, 'data', 'final')

# Cr√©er les dossiers si n√©cessaire
os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)
os.makedirs(FINAL_DATA_PATH, exist_ok=True)

# Initialiser Spark
print("üöÄ D√©marrage du pipeline ETL Electio-Analytics")
print("=" * 60)
spark = create_spark_session('TADA_ElectioAnalytics')


def process_elections_data():
    """
    Traite les donn√©es des √©lections municipales 2020
    """
    print("\nüìä Traitement des donn√©es √©lectorales...")
    
    elections_file = os.path.join(RAW_DATA_PATH, 'DT PRINCIPAL elections-municipales-2020-resultats-du-2eme-tour.csv')
    
    # Extraction
    df_elections = read_csv(spark, elections_file, sep=';')
    print(f"  ‚Üí {df_elections.count()} lignes charg√©es (bureaux de vote)")
    
    # Transformation
    df_elections = clean_column_names(df_elections)
    df_elections = remove_duplicates(df_elections)
    
    print(f"  ‚Üí Aper√ßu des colonnes: {', '.join(df_elections.columns[:15])}")
    
    # Identifier les colonnes importantes
    code_commune_col = None
    nom_commune_col = None
    nom_candidat_col = None
    prenom_candidat_col = None
    code_nuance_col = None
    
    # Chercher colonne code commune
    for col in df_elections.columns:
        col_lower = col.lower()
        if 'code' in col_lower and 'commune' in col_lower and 'officiel' in col_lower:
            code_commune_col = col
            break
    
    # Chercher colonne nom commune
    for col in df_elections.columns:
        col_lower = col.lower()
        if 'nom' in col_lower and 'commune' in col_lower and 'officiel' in col_lower:
            nom_commune_col = col
            break
    
    # Chercher colonne nom candidat
    for col in df_elections.columns:
        col_lower = col.lower()
        if col_lower == 'nom':
            nom_candidat_col = col
            break
    
    # Chercher colonne pr√©nom candidat
    for col in df_elections.columns:
        col_lower = col.lower()
        if 'prenom' in col_lower or 'pr√©nom' in col_lower:
            prenom_candidat_col = col
            break
    
    # Chercher colonne code nuance
    for col in df_elections.columns:
        col_lower = col.lower()
        if 'code' in col_lower and 'nuance' in col_lower:
            code_nuance_col = col
            break
    
    print(f"  ‚Üí Colonnes identifi√©es:")
    print(f"     - Code commune: {code_commune_col}")
    print(f"     - Nom commune: {nom_commune_col}")
    print(f"     - Nom candidat: {nom_candidat_col}")
    print(f"     - Pr√©nom candidat: {prenom_candidat_col}")
    print(f"     - Code nuance: {code_nuance_col}")
    
    if code_commune_col and nom_commune_col and nom_candidat_col and prenom_candidat_col:
        print(f"  ‚Üí Regroupement par commune + candidat")
        
        # Convertir en Pandas pour l'agr√©gation
        df_pandas = df_elections.toPandas()
        
        # Identifier la colonne des voix
        voix_col = None
        for col in df_pandas.columns:
            col_lower = col.lower()
            if col_lower == 'voix':  # Exactement "voix"
                voix_col = col
                break
        
        if not voix_col:
            # Chercher colonne contenant "voix" mais pas "exp" ou "%"
            for col in df_pandas.columns:
                col_lower = col.lower()
                if 'voix' in col_lower and 'exp' not in col_lower and '%' not in col_lower:
                    voix_col = col
                    break
        
        print(f"  ‚Üí Colonne des voix: {voix_col}")
        
        # V√©rifier que les colonnes existent
        if voix_col not in df_pandas.columns:
            print(f"  ‚ö†Ô∏è Erreur: colonne {voix_col} introuvable")
            return df_elections
        
        # Agr√©gation: code + nom commune + nom + pr√©nom candidat + code nuance ‚Üí somme des voix
        group_cols = [code_commune_col, nom_commune_col, nom_candidat_col, prenom_candidat_col]
        if code_nuance_col:
            group_cols.append(code_nuance_col)
        
        print(f"  ‚Üí Groupement par: {group_cols}")
        df_aggregated = df_pandas.groupby(group_cols, as_index=False)[voix_col].sum()
        
        # Cr√©er la colonne orientation politique
        if code_nuance_col and code_nuance_col in df_aggregated.columns:
            print(f"  ‚Üí Ajout de la colonne 'orientation_politique'")
            
            def categoriser_orientation(code):
                if pd.isna(code):
                    return 'INCONNU'
                code = str(code).strip().upper()
                
                # GAUCHE
                if code in ['LEXG', 'LCOP', 'LCOM', 'LSOC', 'LVEC', 'LDVG', 'LUG']:
                    return 'GAUCHE'
                # DROITE
                elif code in ['LCMD', 'LMMD', 'LMAJ', 'LDVD', 'LFN', 'LEXD']:
                    return 'DROITE'
                # CENTRE
                elif code in ['DIV', 'REG']:
                    return 'CENTRE'
                else:
                    return 'AUTRE'
            
            df_aggregated['orientation_politique'] = df_aggregated[code_nuance_col].apply(categoriser_orientation)
            print(f"  ‚úì Orientations ajout√©es:")
            print(df_aggregated['orientation_politique'].value_counts())
        
        print(f"  ‚úì Bureaux regroup√©s: {len(df_pandas)} lignes (bureaux √ó candidats) ‚Üí {len(df_aggregated)} lignes (communes √ó candidats)")
        
        # Sauvegarder
        output_path = os.path.join(PROCESSED_DATA_PATH, 'elections_2020_clean.csv')
        df_aggregated.to_csv(output_path, index=False, encoding='utf-8')
        
        print(f"  ‚úì Donn√©es √©lectorales trait√©es et sauvegard√©es")
        print(f"  ‚Üí Format: Code Commune | Nom Commune | Nom | Pr√©nom | Voix\n")
        print(df_aggregated.head(10))
        
        # Convertir en Spark DataFrame pour retourner
        df_elections = spark.createDataFrame(df_aggregated)
    else:
        print(f"  ‚ö†Ô∏è Colonne commune non trouv√©e, sauvegarde sans agr√©gation")
        output_path = os.path.join(PROCESSED_DATA_PATH, 'elections_2020_clean.csv')
        df_elections.toPandas().to_csv(output_path, index=False, encoding='utf-8')
        print(f"  ‚úì Donn√©es √©lectorales trait√©es et sauvegard√©es")
    
    return df_elections


def process_population_data():
    """
    Traite les donn√©es de population par tranche d'√¢ge
    Garde uniquement 2016 et 2022, puis interpole pour 2020
    """
    print("\nüë• Traitement des donn√©es de population...")
    
    pop_file = os.path.join(RAW_DATA_PATH, 'evolution-de-la-population-par-tranches-dage-dans-lherault.csv')
    
    df_pop = read_csv(spark, pop_file, sep=';')
    print(f"  ‚Üí {df_pop.count()} lignes charg√©es")
    
    df_pop = clean_column_names(df_pop)
    df_pop = remove_duplicates(df_pop)
    
    # Convertir en Pandas pour faciliter le traitement
    df_pandas = df_pop.toPandas()
    
    # Identifier la colonne ann√©e
    annee_col = None
    for col in df_pandas.columns:
        if 'annee' in col.lower() or 'ann√©e' in col.lower():
            annee_col = col
            break
    
    if annee_col:
        print(f"  ‚Üí Colonne ann√©e: {annee_col}")
        
        # Filtrer uniquement 2016 et 2022
        df_pandas[annee_col] = pd.to_numeric(df_pandas[annee_col], errors='coerce')
        df_filtered = df_pandas[df_pandas[annee_col].isin([2016, 2022])].copy()
        
        print(f"  ‚Üí {len(df_filtered)} lignes conserv√©es (2016 et 2022)")
        
        # Identifier colonnes de regroupement (commune, tranche d'√¢ge, etc.)
        code_commune_col = None
        nom_commune_col = None
        tranche_age_col = None
        population_col = None
        
        for col in df_filtered.columns:
            col_lower = col.lower()
            if 'code' in col_lower and 'geo' in col_lower:
                code_commune_col = col
            elif 'nom' in col_lower and 'commune' in col_lower:
                nom_commune_col = col
            elif 'tranche' in col_lower or 'age' in col_lower:
                tranche_age_col = col
            elif 'population' in col_lower:
                population_col = col
        
        print(f"  ‚Üí Code commune: {code_commune_col}, Nom commune: {nom_commune_col}, Tranche: {tranche_age_col}, Population: {population_col}")
        
        if code_commune_col and nom_commune_col and tranche_age_col and population_col and annee_col:
            # Convertir population en num√©rique
            df_filtered[population_col] = pd.to_numeric(df_filtered[population_col], errors='coerce')
            
            # Pivoter pour avoir 2016 et 2022 en colonnes
            df_pivot = df_filtered.pivot_table(
                index=[code_commune_col, nom_commune_col, tranche_age_col],
                columns=annee_col,
                values=population_col,
                aggfunc='sum'
            ).reset_index()
            
            # Renommer les colonnes d'ann√©e
            df_pivot.columns = [str(col) if col not in [code_commune_col, nom_commune_col, tranche_age_col] else col for col in df_pivot.columns]
            
            # Interpolation lin√©aire pour 2020
            # 2020 = 2016 + (2022 - 2016) * (2020-2016)/(2022-2016)
            # 2020 = 2016 + (2022 - 2016) * 4/6
            if '2016' in df_pivot.columns and '2022' in df_pivot.columns:
                df_pivot['2020_estimation'] = df_pivot['2016'] + (df_pivot['2022'] - df_pivot['2016']) * (4/6)
                
                print(f"  ‚úì Estimation 2020 calcul√©e par interpolation lin√©aire")
                print(f"  ‚Üí Formule: 2020_estimation = 2016 + (2022 - 2016) √ó 0.667")
                
                # Garder uniquement les colonnes n√©cessaires (sans 2016 et 2022)
                cols = [code_commune_col, nom_commune_col, tranche_age_col, '2020_estimation']
                df_result = df_pivot[cols]
                
                # Regrouper certaines tranches d'√¢ges
                print(f"  ‚Üí Regroupement des tranches d'√¢ges...")
                
                # Cr√©er un mapping pour regrouper les tranches
                def regrouper_tranche(tranche):
                    tranche_str = str(tranche).strip()
                    if '30' in tranche_str and '44' in tranche_str:
                        return '30-59'
                    elif '45' in tranche_str and '59' in tranche_str:
                        return '30-59'
                    elif '60' in tranche_str and '74' in tranche_str:
                        return '60+'
                    elif '75' in tranche_str and '89' in tranche_str:
                        return '60+'
                    elif '90' in tranche_str:
                        return '60+'
                    else:
                        return tranche_str
                
                # Appliquer le regroupement
                df_result['tranche_regroupee'] = df_result[tranche_age_col].apply(regrouper_tranche)
                
                # Agr√©ger par commune et nouvelle tranche
                df_result = df_result.groupby([code_commune_col, nom_commune_col, 'tranche_regroupee'], as_index=False)['2020_estimation'].sum()
                
                # Convertir en entier (arrondir)
                df_result['2020_estimation'] = df_result['2020_estimation'].round().astype(int)
                
                # Renommer la colonne
                df_result.rename(columns={'tranche_regroupee': tranche_age_col}, inplace=True)
                
                print(f"  ‚úì Tranches d'√¢ges regroup√©es:")
                print(f"     - 30-44 + 45-59 ‚Üí 30-59")
                print(f"     - 60-74 + 75-89 + 90+ ‚Üí 60+")
                print(f"  ‚úì Populations arrondies en entiers")
                
                # Sauvegarder
                output_path = os.path.join(PROCESSED_DATA_PATH, 'population_age_clean.csv')
                df_result.to_csv(output_path, index=False, encoding='utf-8')
                
                print(f"  ‚úì Donn√©es de population trait√©es et sauvegard√©es")
                print(f"  ‚Üí Aper√ßu:\n")
                print(df_result.head(10))
                
                # Convertir en Spark DataFrame
                df_pop = spark.createDataFrame(df_result)
            else:
                print("  ‚ö†Ô∏è Colonnes 2016 ou 2022 manquantes")
        else:
            print("  ‚ö†Ô∏è Colonnes essentielles non trouv√©es")
    else:
        print("  ‚ö†Ô∏è Colonne ann√©e non trouv√©e")
    
    return df_pop


def process_education_data():
    """
    Traite les donn√©es de niveau de dipl√¥me
    Regroupe hommes et femmes ensemble par commune et dipl√¥me
    """
    print("\nüéì Traitement des donn√©es d'√©ducation...")
    
    edu_file = os.path.join(RAW_DATA_PATH, 'niveau-de-diplome-de-la-population-herault.csv')
    
    df_edu = read_csv(spark, edu_file, sep=';')
    print(f"  ‚Üí {df_edu.count()} lignes charg√©es")
    
    df_edu = clean_column_names(df_edu)
    df_edu = remove_duplicates(df_edu)
    
    # Convertir en Pandas pour faciliter le traitement
    df_pandas = df_edu.toPandas()
    
    # Identifier les colonnes importantes
    code_commune_col = None
    nom_commune_col = None
    diplome_col = None
    population_col = None
    
    for col in df_pandas.columns:
        col_lower = col.lower()
        if 'codgeo' in col_lower or ('code' in col_lower and 'geo' in col_lower):
            code_commune_col = col
        elif 'nom' in col_lower and 'commune' in col_lower:
            nom_commune_col = col
        elif 'diplome' in col_lower or 'dipl√¥me' in col_lower:
            diplome_col = col
        elif 'population' in col_lower:
            population_col = col
    
    print(f"  ‚Üí Code commune: {code_commune_col}")
    print(f"  ‚Üí Nom commune: {nom_commune_col}")
    print(f"  ‚Üí Dipl√¥me: {diplome_col}")
    print(f"  ‚Üí Population: {population_col}")
    
    if code_commune_col and nom_commune_col and diplome_col and population_col:
        # Convertir population en num√©rique
        df_pandas[population_col] = pd.to_numeric(df_pandas[population_col], errors='coerce')
        
        # Regrouper par commune et dipl√¥me (sans distinction de sexe)
        print(f"  ‚Üí Regroupement hommes + femmes par commune et dipl√¥me...")
        df_aggregated = df_pandas.groupby([code_commune_col, nom_commune_col, diplome_col], as_index=False)[population_col].sum()
        
        # Regrouper les niveaux de dipl√¥me en cat√©gories
        print(f"  ‚Üí Regroupement des niveaux de dipl√¥me en cat√©gories...")
        
        def regrouper_diplome(diplome):
            diplome_str = str(diplome).lower().strip()
            
            # Brevet ou sans diplome
            if any(x in diplome_str for x in ['bepc', 'brevet', 'dnb', 'sans diplome', 'cep']):
                return 'Brevet ou sans diplome'
            # Bac ou CAP/BEP
            elif any(x in diplome_str for x in ['bac', 'cap', 'bep', 'brevet pro']):
                if 'bac + 2' not in diplome_str and 'bac + 3' not in diplome_str and 'bac + 4' not in diplome_str and 'bac + 5' not in diplome_str:
                    return 'Bac ou CAP/BEP'
            # Enseignement sup bac +2 √† 4
            if 'bac + 2' in diplome_str or 'bac + 3' in diplome_str or 'bac + 4' in diplome_str:
                return 'Enseignement sup de niveau bac +2 √† 4'
            # Enseignement sup bac +5 ou plus
            elif 'bac + 5' in diplome_str or 'bac +5' in diplome_str:
                return 'Enseignement sup de niveau bac +5 ou plus'
            
            return diplome  # Garder tel quel si pas de correspondance
        
        # Appliquer le regroupement
        df_aggregated['diplome_regroupe'] = df_aggregated[diplome_col].apply(regrouper_diplome)
        
        # Agr√©ger √† nouveau par commune et dipl√¥me regroup√©
        df_aggregated = df_aggregated.groupby([code_commune_col, nom_commune_col, 'diplome_regroupe'], as_index=False)[population_col].sum()
        
        print(f"  ‚úì Dipl√¥mes regroup√©s en 4 cat√©gories:")
        print(f"     - Brevet ou sans diplome")
        print(f"     - Bac ou CAP/BEP")
        print(f"     - Enseignement sup de niveau bac +2 √† 4")
        print(f"     - Enseignement sup de niveau bac +5 ou plus")
        
        # Garder uniquement les 4 colonnes n√©cessaires et renommer
        df_result = df_aggregated[[code_commune_col, nom_commune_col, 'diplome_regroupe', population_col]].copy()
        df_result.columns = ['code_commune', 'nom_commune', 'niveau_diplome', 'population_totale']
        
        print(f"  ‚úì {len(df_pandas)} lignes ‚Üí {len(df_result)} lignes (hommes + femmes agr√©g√©s)")
        print(f"  ‚úì 4 colonnes conserv√©es: code_commune, nom_commune, niveau_diplome, population_totale")
        
        # Sauvegarder
        output_path = os.path.join(PROCESSED_DATA_PATH, 'education_clean.csv')
        df_result.to_csv(output_path, index=False, encoding='utf-8')
        
        print(f"  ‚úì Donn√©es d'√©ducation trait√©es et sauvegard√©es")
        print(f"  ‚Üí Aper√ßu:\n")
        print(df_result.head(10))
        
        # Convertir en Spark DataFrame
        df_edu = spark.createDataFrame(df_result)
    else:
        print("  ‚ö†Ô∏è Colonnes essentielles non trouv√©es, sauvegarde sans traitement")
        output_path = os.path.join(PROCESSED_DATA_PATH, 'education_clean.csv')
        df_pandas.to_csv(output_path, index=False, encoding='utf-8')
    
    return df_edu


def process_employment_data():
    """
    Traite les donn√©es de population active
    Calcule le taux d'emploi et le taux de ch√¥mage par commune
    """
    print("\nüíº Traitement des donn√©es d'emploi...")
    
    emp_file = os.path.join(RAW_DATA_PATH, 'population-active-herault.csv')
    
    df_emp = read_csv(spark, emp_file, sep=';')
    print(f"  ‚Üí {df_emp.count()} lignes charg√©es")
    
    df_emp = clean_column_names(df_emp)
    df_emp = remove_duplicates(df_emp)
    
    # Convertir en Pandas pour faciliter le traitement
    df_pandas = df_emp.toPandas()
    
    # Identifier les colonnes importantes
    code_commune_col = None
    nom_commune_col = None
    pop_active_col = None
    pop_active_occupee_col = None
    
    for col in df_pandas.columns:
        col_lower = col.lower()
        if 'codgeo' in col_lower or ('code' in col_lower and 'geo' in col_lower):
            code_commune_col = col
        elif 'nom' in col_lower and 'commune' in col_lower:
            nom_commune_col = col
        elif 'actives_occupees' in col_lower or ('actives' in col_lower and 'occupees' in col_lower):
            pop_active_occupee_col = col
        elif 'actives' in col_lower and 'occupees' not in col_lower:
            pop_active_col = col
    
    print(f"  ‚Üí Code commune: {code_commune_col}")
    print(f"  ‚Üí Nom commune: {nom_commune_col}")
    print(f"  ‚Üí Population active: {pop_active_col}")
    print(f"  ‚Üí Population active occup√©e: {pop_active_occupee_col}")
    
    if code_commune_col and nom_commune_col and pop_active_col and pop_active_occupee_col:
        # Convertir en num√©rique
        df_pandas[pop_active_col] = pd.to_numeric(df_pandas[pop_active_col], errors='coerce')
        df_pandas[pop_active_occupee_col] = pd.to_numeric(df_pandas[pop_active_occupee_col], errors='coerce')
        
        # Cr√©er un nouveau dataframe avec les colonnes n√©cessaires
        df_result = df_pandas[[code_commune_col, nom_commune_col, pop_active_col, pop_active_occupee_col]].copy()
        
        # Calculer le taux de ch√¥mage: (population active - population active occup√©e) / population active
        df_result['taux_chomage'] = ((df_result[pop_active_col] - df_result[pop_active_occupee_col]) / df_result[pop_active_col] * 100).round(2)
        
        # Garder uniquement les colonnes n√©cessaires
        df_result = df_result[[code_commune_col, nom_commune_col, 'taux_chomage']].copy()
        df_result.columns = ['code_commune', 'nom_commune', 'taux_chomage']
        
        print(f"  ‚úì Calcul du taux de ch√¥mage effectu√©:")
        print(f"     - Formule: ((Population active - Population active occup√©e) / Population active) √ó 100")
        print(f"  ‚Üí Aper√ßu des statistiques:")
        print(f"     - Taux de ch√¥mage moyen: {df_result['taux_chomage'].mean():.2f}%")
        print(f"  ‚úì 3 colonnes conserv√©es: code_commune, nom_commune, taux_chomage")
        
        # Sauvegarder
        output_path = os.path.join(PROCESSED_DATA_PATH, 'emploi_clean.csv')
        df_result.to_csv(output_path, index=False, encoding='utf-8')
        
        print(f"  ‚úì Donn√©es d'emploi trait√©es et sauvegard√©es")
        print(f"  ‚Üí Aper√ßu:\n")
        print(df_result.head(10))
        
        # Convertir en Spark DataFrame
        df_emp = spark.createDataFrame(df_result)
    else:
        print("  ‚ö†Ô∏è Colonnes essentielles non trouv√©es, sauvegarde sans traitement")
        output_path = os.path.join(PROCESSED_DATA_PATH, 'emploi_clean.csv')
        df_pandas.to_csv(output_path, index=False, encoding='utf-8')
    
    return df_emp


def process_sociopro_data():
    """
    Traite les donn√©es de cat√©gories socioprofessionnelles
    Regroupe les CSP en 4 grandes cat√©gories
    """
    print("\nüëî Traitement des donn√©es socioprofessionnelles...")
    
    socio_file = os.path.join(RAW_DATA_PATH, 'population par categorie socioprofessionelle.csv')
    
    df_socio = read_csv(spark, socio_file, sep=';')
    print(f"  ‚Üí {df_socio.count()} lignes charg√©es")
    
    df_socio = clean_column_names(df_socio)
    df_socio = remove_duplicates(df_socio)
    
    # Convertir en Pandas pour faciliter le traitement
    df_pandas = df_socio.toPandas()
    
    # Identifier les colonnes importantes
    code_commune_col = None
    nom_commune_col = None
    csp_col = None
    population_col = None
    
    for col in df_pandas.columns:
        col_lower = col.lower()
        if 'codgeo' in col_lower or ('code' in col_lower and 'geo' in col_lower):
            code_commune_col = col
        elif 'nom' in col_lower and 'commune' in col_lower:
            nom_commune_col = col
        elif 'csp' in col_lower or 'socioprofession' in col_lower or 'categorie' in col_lower:
            csp_col = col
        elif 'population' in col_lower:
            population_col = col
    
    print(f"  ‚Üí Code commune: {code_commune_col}")
    print(f"  ‚Üí Nom commune: {nom_commune_col}")
    print(f"  ‚Üí CSP: {csp_col}")
    print(f"  ‚Üí Population: {population_col}")
    
    if code_commune_col and nom_commune_col and csp_col and population_col:
        # Convertir population en num√©rique
        df_pandas[population_col] = pd.to_numeric(df_pandas[population_col], errors='coerce')
        
        # Regrouper les CSP en 4 grandes cat√©gories
        print(f"  ‚Üí Regroupement des CSP en 4 grandes cat√©gories...")
        
        def regrouper_csp(csp):
            csp_str = str(csp).lower().strip()
            
            # Ind√©pendants = Artisans, Comm., Chefs entr. + Agriculteurs exploitants
            if 'artisan' in csp_str or 'comm.' in csp_str or 'chef' in csp_str or 'agriculteur' in csp_str or 'exploitant' in csp_str:
                return 'Ind√©pendants'
            # Actifs qualifi√©s = Prof. interm√©diaires + Cadres, Prof. intel. sup.
            elif 'prof' in csp_str and 'interm√©diaire' in csp_str:
                return 'Actifs qualifi√©s'
            elif 'cadre' in csp_str or 'intel' in csp_str:
                return 'Actifs qualifi√©s'
            # Inactifs = Retrait√©s + Autres
            elif 'retrait√©' in csp_str or 'retraite' in csp_str or 'autre' in csp_str:
                return 'Inactifs'
            # Actifs populaires = Ouvriers + Employ√©s
            elif 'ouvrier' in csp_str or 'employ√©' in csp_str or 'employe' in csp_str:
                return 'Actifs populaires'
            else:
                return 'Autres'  # Pour les CSP non identifi√©es
        
        # Appliquer le regroupement
        df_pandas['csp_regroupee'] = df_pandas[csp_col].apply(regrouper_csp)
        
        # Agr√©ger par commune et CSP regroup√©e
        df_aggregated = df_pandas.groupby([code_commune_col, nom_commune_col, 'csp_regroupee'], as_index=False)[population_col].sum()
        
        # Arrondir les populations en entiers
        df_aggregated[population_col] = df_aggregated[population_col].round().astype(int)
        
        print(f"  ‚úì CSP regroup√©es en 4 cat√©gories:")
        print(f"     - Ind√©pendants = Artisans, Comm., Chefs entr. + Agriculteurs exploitants")
        print(f"     - Actifs qualifi√©s = Prof. interm√©diaires + Cadres, Prof. intel. sup.")
        print(f"     - Inactifs = Retrait√©s + Autres")
        print(f"     - Actifs populaires = Ouvriers + Employ√©s")
        print(f"  ‚úì Populations arrondies en entiers")
        
        # Garder uniquement les colonnes n√©cessaires et renommer
        df_result = df_aggregated[[code_commune_col, nom_commune_col, 'csp_regroupee', population_col]].copy()
        df_result.columns = ['code_commune', 'nom_commune', 'categorie_sociopro', 'population']
        
        print(f"  ‚úì {len(df_pandas)} lignes ‚Üí {len(df_result)} lignes (CSP regroup√©es)")
        print(f"  ‚úì 4 colonnes conserv√©es: code_commune, nom_commune, categorie_sociopro, population")
        
        # Sauvegarder
        output_path = os.path.join(PROCESSED_DATA_PATH, 'sociopro_clean.csv')
        df_result.to_csv(output_path, index=False, encoding='utf-8')
        
        print(f"  ‚úì Donn√©es socioprofessionnelles trait√©es et sauvegard√©es")
        print(f"  ‚Üí Aper√ßu:\n")
        print(df_result.head(10))
        
        # Convertir en Spark DataFrame
        df_socio = spark.createDataFrame(df_result)
    else:
        print("  ‚ö†Ô∏è Colonnes essentielles non trouv√©es, sauvegarde sans traitement")
        output_path = os.path.join(PROCESSED_DATA_PATH, 'sociopro_clean.csv')
        df_pandas.to_csv(output_path, index=False, encoding='utf-8')
    
    return df_socio


def process_communes_data():
    """
    Traite les donn√©es de r√©f√©rence des communes
    """
    print("\nüó∫Ô∏è  Traitement des donn√©es des communes...")
    
    communes_file = os.path.join(RAW_DATA_PATH, 'communes-france-2025 (1).csv')
    
    df_communes = read_csv(spark, communes_file, sep=',')
    print(f"  ‚Üí {df_communes.count()} lignes charg√©es")
    
    df_communes = clean_column_names(df_communes)
    df_communes = remove_duplicates(df_communes)
    
    # Filtrer pour l'H√©rault (d√©partement 34)
    if 'code_departement' in df_communes.columns:
        df_communes = df_communes.filter(df_communes.code_departement == '34')
    elif 'dep' in df_communes.columns:
        df_communes = df_communes.filter(df_communes.dep == '34')
    
    print(f"  ‚Üí {df_communes.count()} communes dans l'H√©rault")
    
    output_path = os.path.join(PROCESSED_DATA_PATH, 'communes_herault_clean.csv')
    df_communes.toPandas().to_csv(output_path, index=False, encoding='utf-8')
    
    print(f"  ‚úì Donn√©es des communes trait√©es et sauvegard√©es")
    return df_communes


if __name__ == "__main__":
    try:
        # Ex√©cuter tous les traitements
        df_elections = process_elections_data()
        df_population = process_population_data()
        df_education = process_education_data()
        df_employment = process_employment_data()
        df_sociopro = process_sociopro_data()
        #df_communes = process_communes_data()
        
        print("\n" + "=" * 60)
        print("‚úÖ Pipeline ETL termin√© avec succ√®s!")
        print(f"üìÅ Donn√©es trait√©es disponibles dans: {PROCESSED_DATA_PATH}")
        print("=" * 60)
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'ex√©cution du pipeline: {str(e)}")
        raise
    finally:
        spark.stop()
        print("\nüõë Session Spark ferm√©e")

